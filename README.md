# Optimization Tools

Algorithms I learned at TUM


# Taylor Test
    Credit to Dr. Johannes Milz
    

## Unconstrained algorithms
    -   Gradient descent [1]:
        Requirements:
        -   f is required to be continously differentiable on a domain $U \subset \mathbb{R}^n$
        -   If f is coercive, than the Powell Wolfe stepsizing is feasible
    
    -global Newton method [1]:
        Requirement:
        -   f is twice continously differentiable
        
        
    #Global quasi newton method: BFGS update
        Alg. 13.9 in [1]


#   References
    [1]     https://link.springer.com/book/10.1007/978-3-0346-0654-7
